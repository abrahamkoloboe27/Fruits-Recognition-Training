# üçé Pr√©paration des Donn√©es et Entra√Ænement des Mod√®les ü§ñ

---

### **Introduction**  
Ce projet vise √† entra√Æner et comparer plusieurs mod√®les de deep learning pour classer les images issues du dataset [Fruits 360 (100x100)](https://github.com/fruits-360/fruits-360-100x100). L‚Äôobjectif est de s√©lectionner le **mod√®le le plus performant** selon plusieurs m√©triques pour l‚Äôenvoyer en production. Ce README documente la **pr√©paration des donn√©es**, l‚Äô**entra√Ænement des mod√®les** et la **m√©thodologie utilis√©e**.

---

### **Table des Mati√®res**  
1. [Dataset et Pr√©paration](#dataset-et-pr√©paration) üçá  
2. [Data Augmentation avec Albumentations](#data-augmentation-avec-albumentations) üìà  
3. [Architecture des Mod√®les Entra√Æn√©s](#architecture-des-mod√®les-entrain√©s) üèóÔ∏è  
4. [Gestion des Callbacks et Enregistrement des Mod√®les](#gestion-des-callbacks-et-enregistrement-des-mod√®les) üíæ  
5. [Suivi des Performances et Calcul des M√©triques](#suivi-des-performances-et-calcul-des-m√©triques) üìä  
6. [R√©f√©rences et Ressources Utiles](#r√©f√©rences-et-ressources-utiles) üìö

---

### **1. Dataset et Pr√©paration**  
Nous utilisons le **dossier `Training`** du dataset Fruits 360, contenant des images de diff√©rentes classes de fruits.  
- **Split des donn√©es :** 75 % pour l‚Äôentra√Ænement et 25 % pour la validation.  
- **Pourquoi ce split ?** Il permet de s‚Äôassurer que le mod√®le peut bien g√©n√©raliser sur des donn√©es non vues.

#### **Commandes pour Pr√©parer le Dataset**  

```python
def load_data(data_dir, validation_split=0.25, seed=1337, 
                image_size=(100, 100), batch_size=128, 
                label_mode='int'):
    """Load and split the data into training and validation sets."""
    logging.info(f"Loading data from {data_dir}")
    train_ds, val_ds = keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=validation_split,
        subset="both",
        seed=seed,
        image_size=image_size,
        batch_size=batch_size,
        label_mode=label_mode
    )
    return train_ds, val_ds
# Load data
train_ds, val_ds = load_data("data/Training")
```

---

### **2. Data Augmentation avec Albumentations**  
Pour **enrichir le dataset** et √©viter l‚Äôoverfitting, nous avons appliqu√© de la **data augmentation** avec [Albumentations](https://albumentations.ai/).  
**Transformations appliqu√©es :**  
- **Rotation** al√©atoire entre -15¬∞ et +15¬∞ üîÑ  
- **Flip horizontal** ‚ÜîÔ∏è  
- **Modification de la luminosit√© et du contraste** üåû

#### **Extrait de Code : Data Augmentation**
```python
import albumentations as A

# D√©finition de l'augmentation
transforms = [
        A.RandomRotate90(p=1.0),
        A.Transpose(p=1.0),
        A.VerticalFlip(p=1.0),
        A.HorizontalFlip(p=1.0),
        A.RandomBrightnessContrast(brightness_limit=0.5, 
        contrast_limit=0.5, p=1.0),
    ]
```

**Pourquoi utiliser Albumentations ?**  
- Optimis√© pour des performances rapides. üöÄ  
- Prend en charge **PyTorch et TensorFlow/Keras**. üß†  

Pour en savoir plus : [Guide Albumentations](https://albumentations.ai/docs/).

---

### **3. Architecture des Mod√®les Entra√Æn√©s**  
Nous avons test√© 4 architectures :  
- **CNN Custom** (baseline simple)  
- **ResNet50**  
- **VGG16**  
- **EfficientNetB0**  

<div style="text-align: center;">
<table style="margin: auto;">
<tr>
<th>CNN Custom</th>
<th>EfficientNet</th>
<th>ResNet</th>
<th>VGG16</th>
</tr>
<tr>
<td>
<img src="images-models/svg/CNN.svg" alt="CNN Custom" width="100"/>
</td>
<td>
<img src="images-models/svg/EfficientNet-Base.svg" alt="EfficientNet" width="100"/>
</td>
<td>
<img src="images-models/svg/ResNet-Fine-Tuning.svg" alt="ResNet" width="100"/>
</td>
<td>
<img src="images-models/svg/VGG16-Fine-Tuning.svg" alt="VGG16" width="100"/>
</td>
</tr>
</table>
</div>

#### **Pourquoi plusieurs mod√®les ?**  
- **ResNet** et **EfficientNet** offrent des performances √©lev√©es sur des t√¢ches de classification.  
- **VGG16** est plus simple mais toujours comp√©titif.  
- **CNN Custom** permet d‚Äô√©tablir une **baseline** √† partir de laquelle comparer les performances.

**Code : Initialisation des Mod√®les Pr√©-entrain√©s**  
```python
def create_efficientnet_model(num_classes):
    """Create an EfficientNet model."""
    logging.info("Creating EfficientNet model")
    base_model = EfficientNetB0(weights='imagenet', 
    include_top=False, input_shape=(100, 100, 3))
    base_model.trainable = False

    model = keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(name='global_avg_pooling'),
        layers.Dense(num_classes*3, activation='relu', name='dense_1'),
        layers.BatchNormalization(name='batch_norm_1'),
        layers.Dropout(0.2, name='dropout_1'),
        layers.Dense(num_classes*2, activation='relu', name='dense_2'),
        layers.BatchNormalization(name='batch_norm_2'),
        layers.Dropout(0.2, name='dropout_2'),
        layers.Dense(num_classes, activation='softmax', name='output_layer')
    ])
    return model
create_efficientnet_model(num_classes)
```

---

### **4. Gestion des Callbacks et Enregistrement des Mod√®les**  
Nous avons utilis√© un **callback Keras** pour enregistrer uniquement le **meilleur mod√®le** sur la validation.

#### **Extrait de Code : Callback**
```python
imoort keras
logging.info(f"Training {model_name} model")
callbacks = [
        keras.callbacks.ModelCheckpoint(
            f"models/best_model_{model_name}.keras", 
            save_best_only=True, monitor="val_acc", mode="max"
        ),
        keras.callbacks.EarlyStopping(monitor='val_acc', patience=patience, 
        mode="max", restore_best_weights=True),
        keras.callbacks.CSVLogger(f'artefacts/training_log_{model_name}.csv')
]
```

**Pourquoi utiliser un callback ?**  
- Automatisation de l‚Äôenregistrement du **meilleur mod√®le** pour √©viter les surentra√Ænements.  
- Permet de **recharger facilement** le mod√®le pour une utilisation future.  

---

### **5. Suivi des Performances et Calcul des M√©triques**  
Apr√®s l‚Äôentra√Ænement, nous avons calcul√© plusieurs m√©triques pour √©valuer les performances sur la validation :  
- **F1-Score** : √âquilibre entre pr√©cision et rappel.  
- **AUC (Area Under Curve)** : Mesure de la capacit√© du mod√®le √† distinguer les classes.  
- **Pr√©cision et Rappel** : Pour √©valuer la qualit√© des pr√©dictions.

#### **Extrait de Code : Calcul des M√©triques**
```python
from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score

y_pred = model.predict(X_val)
f1 = f1_score(y_val, y_pred.argmax(axis=1), average='weighted')
auc = roc_auc_score(y_val, y_pred, multi_class='ovo')
precision = precision_score(y_val, y_pred.argmax(axis=1), average='weighted')
recall = recall_score(y_val, y_pred.argmax(axis=1), average='weighted')

print(f"F1: {f1}, AUC: {auc}, Precision: {precision}, Recall: {recall}")
```

---

### **6. R√©f√©rences et Ressources Utiles**  
- **Albumentations :** [Documentation](https://albumentations.ai/docs/)  
- **Keras Callbacks :** [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/)  
- **ResNet et Fine-Tuning :** [Article de r√©f√©rence](https://arxiv.org/abs/1512.03385)  
- **EfficientNet :** [Article de recherche](https://arxiv.org/abs/1905.11946)  
- **Introduction aux m√©triques ML :** [Guide Sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

### **Comment R√©pliquer l‚ÄôEntra√Ænement ?**  
Pour reproduire l‚Äôentra√Ænement :  
1. **Cloner le repo :**  
   ```bash
   git https://github.com/abrahamkoloboe27/Machine-Learning-En-Production-LinkedIn.git /data
   cd data
   ```
2. **Installer les d√©pendances :**  
   ```bash
   pip install -r requirements.txt
   ```
3. **Lancer l‚Äôentra√Ænement :**  
   ```bash
   python main.py
   ```

---

### **Conclusion**  
Ce README documente toutes les √©tapes de **pr√©paration des donn√©es** et **entra√Ænement des mod√®les**. Chaque d√©cision technique a √©t√© justifi√©e pour **assurer la qualit√© du mod√®le final**. N'h√©sitez pas √† explorer les **r√©f√©rences fournies** pour approfondir votre compr√©hension des concepts utilis√©s.

